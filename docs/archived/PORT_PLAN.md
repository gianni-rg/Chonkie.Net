# ðŸ¦› Chonkie.Net - Port Plan

**Version:** 1.1  
**Last Updated:** December 16, 2025  
**Status:** Phase 6 Complete - C# 14 & .NET 10 Enhancements Complete (65% overall progress)

---

## Table of Contents

- [Executive Summary](#executive-summary)
- [1. Project Architecture Overview](#1-project-architecture-overview)
- [2. Technology Stack](#2-technology-stack)
- [3. Core Implementation Details](#3-core-implementation-details)
- [4. Performance Optimizations](#4-performance-optimizations)
- [5. Integration Strategy](#5-integration-strategy)
- [6. Migration Phases](#6-migration-phases)
- [7. Testing Strategy](#7-testing-strategy)
- [8. Documentation Requirements](#8-documentation-requirements)
- [9. NuGet Package Strategy](#9-nuget-package-strategy)
- [10. Key Differences from Python](#10-key-differences-from-python)
- [11. Success Criteria](#11-success-criteria)
- [12. Progress Tracking](#12-progress-tracking)

---

## Executive Summary

Chonkie is a lightweight, high-performance text chunking library for RAG (Retrieval-Augmented Generation) applications. The port to .NET/C# will maintain the same philosophy: minimal dependencies, high performance, and extensive integrations.

### Key Metrics (Python Version 1.5.0)
- **Package Size:** 505KB wheel (vs 1-12MB for alternatives)
- **Installed Size:** 49MB (vs 80-171MB for alternatives)
- **Performance:** 33x faster for token chunking, 2x faster for sentence chunking
- **Integrations:** 35+ integrations across the ecosystem (9 vector DBs, 10 embedding providers)
- **Languages Supported:** 56 languages out-of-the-box

### Port Goals
- âœ… Feature parity with Python version
- âœ… Performance within 10% (or better)
- âœ… Maintain lightweight philosophy
- âœ… Leverage .NET ecosystem advantages
- âœ… Provide excellent developer experience

---

## 1. Project Architecture Overview

### 1.1 Core Components to Port

#### **Types** - Core data structures
- `Chunk` - Main chunking unit with metadata (text, indices, token count, embeddings)
- `Document` - Document container with chunks and metadata
- `Sentence` - Sentence representation with position tracking
- `RecursiveLevel` & `RecursiveRules` - Hierarchical chunking configuration
- `LanguageConfig`, `MergeRule`, `SplitRule` - Code chunking configuration
- `MarkdownDocument`, `MarkdownTable`, `MarkdownCode` - Markdown processing types

#### **Tokenizers** - Text tokenization abstraction
- `ITokenizer` interface (port of TokenizerProtocol)
- `CharacterTokenizer` - Character-level tokenization
- `WordTokenizer` - Word-level tokenization
- `ByteTokenizer` - Byte-level tokenization
- `AutoTokenizer` - Factory pattern for automatic tokenizer loading
- Integration adapters for:
  - Microsoft.ML.Tokenizers (HuggingFace compatible)
  - SharpToken (tiktoken/OpenAI)
  - Custom token counters

#### **Chunkers** - Core chunking implementations
- `BaseChunker` - Abstract base class with batch processing
- `TokenChunker` - Fixed-size token chunks with overlap
- `SentenceChunker` - Sentence-boundary aware chunking
- `RecursiveChunker` - Hierarchical splitting with custom rules
- `SemanticChunker` - Similarity-based semantic chunking
- `LateChunker` - Embed-then-chunk approach
- `CodeChunker` - Code-aware structural chunking
- `NeuralChunker` - ML model-based chunking
- `SlumberChunker` - LLM-guided agentic chunking
- `TableChunker` - Table-aware chunking

#### **Embeddings** - Text embedding providers
- `IEmbeddings` interface
- `AutoEmbeddings` - Factory for automatic provider loading
- Provider implementations:
  - `OpenAIEmbeddings` - OpenAI API
  - `AzureOpenAIEmbeddings` - Azure OpenAI Service
  - `CohereEmbeddings` - Cohere API
  - `GeminiEmbeddings` - Google Gemini API
  - `JinaEmbeddings` - Jina AI API
  - `VoyageAIEmbeddings` - Voyage AI API
  - `SentenceTransformerEmbeddings` - Local ONNX models
  - `Model2VecEmbeddings` - Model2Vec models
  - `LiteLLMEmbeddings` - Unified API for 100+ providers (optional)
  - `CatsuEmbeddings` - Unified client for 11+ providers (optional)

#### **Refineries** - Post-processing pipeline
- `IRefinery` interface
- `OverlapRefinery` - Merge overlapping chunks based on similarity
- `EmbeddingsRefinery` - Add embeddings to chunks using any provider

#### **Fetchers** - Data loading
- `IFetcher` interface
- `FileFetcher` - Load text from files and directories

#### **Chefs** - Text preprocessing
- `IChef` interface
- `TextChef` - Basic text preprocessing and cleaning
- `MarkdownChef` - Markdown document processing
- `TableChef` - Table extraction and processing

#### **Pipeline** - Component composition
- `Pipeline` - Fluent API for chaining operations (CHOMP architecture)
- `ComponentRegistry` - Component registration and discovery
- Pipeline stages: Fetcher â†’ Chef â†’ Chunker â†’ Refinery â†’ Porter â†’ Handshake

#### **Handshakes** - Vector DB integrations
- `IHandshake` interface - Unified ingestion interface
- Implementations:
  - `ChromaHandshake` - ChromaDB
  - `QdrantHandshake` - Qdrant
  - `PineconeHandshake` - Pinecone
  - `WeaviateHandshake` - Weaviate
  - `PgvectorHandshake` - PostgreSQL with pgvector
  - `MongoDBHandshake` - MongoDB
  - `ElasticHandshake` - Elasticsearch
  - `TurbopufferHandshake` - Turbopuffer

#### **Porters** - Export functionality
- `IPorter` interface
- `JsonPorter` - JSON file export
- `DatasetsPorter` - HuggingFace datasets export (optional)

#### **Genies** - LLM integrations
- `IGenie` interface - LLM interaction abstraction
- Implementations:
  - `OpenAIGenie` - OpenAI API (compatible with OpenRouter, etc.)
  - `AzureOpenAIGenie` - Azure OpenAI Service
  - `GeminiGenie` - Google Gemini API

---

## 2. Technology Stack

### 2.1 .NET Version
- **Target Framework:** .NET 10.0 (Preview/RC)
- **Language Version:** C# 13
- **Features to Leverage:**
  - Nullable reference types
  - Records and record structs
  - Pattern matching enhancements
  - Init-only properties
  - Global usings
  - File-scoped namespaces
  - C# 13 features as they become available

### 2.2 Project Structure

```
Chonkie.Net/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Chonkie.Core/                 # Core types, interfaces, base classes
â”‚   â”‚   â”œâ”€â”€ Types/
â”‚   â”‚   â”œâ”€â”€ Interfaces/
â”‚   â”‚   â””â”€â”€ Exceptions/
â”‚   â”œâ”€â”€ Chonkie.Tokenizers/           # Tokenization implementations
â”‚   â”‚   â”œâ”€â”€ Character/
â”‚   â”‚   â”œâ”€â”€ Word/
â”‚   â”‚   â””â”€â”€ Auto/
â”‚   â”œâ”€â”€ Chonkie.Chunkers/             # All chunker implementations
â”‚   â”‚   â”œâ”€â”€ Base/
â”‚   â”‚   â”œâ”€â”€ Token/
â”‚   â”‚   â”œâ”€â”€ Sentence/
â”‚   â”‚   â”œâ”€â”€ Recursive/
â”‚   â”‚   â”œâ”€â”€ Semantic/
â”‚   â”‚   â”œâ”€â”€ Late/
â”‚   â”‚   â”œâ”€â”€ Code/
â”‚   â”‚   â”œâ”€â”€ Neural/
â”‚   â”‚   â””â”€â”€ Slumber/
â”‚   â”œâ”€â”€ Chonkie.Embeddings/           # Base embeddings
â”‚   â”‚   â”œâ”€â”€ Chonkie.Embeddings.OpenAI/
â”‚   â”‚   â”œâ”€â”€ Chonkie.Embeddings.Azure/
â”‚   â”‚   â”œâ”€â”€ Chonkie.Embeddings.Cohere/
â”‚   â”‚   â”œâ”€â”€ Chonkie.Embeddings.Gemini/
â”‚   â”‚   â”œâ”€â”€ Chonkie.Embeddings.Jina/
â”‚   â”‚   â”œâ”€â”€ Chonkie.Embeddings.VoyageAI/
â”‚   â”‚   â””â”€â”€ Chonkie.Embeddings.SentenceTransformers/
â”‚   â”œâ”€â”€ Chonkie.Refineries/           # Post-processing
â”‚   â”œâ”€â”€ Chonkie.Fetchers/             # Data loading
â”‚   â”œâ”€â”€ Chonkie.Chefs/                # Preprocessing
â”‚   â”œâ”€â”€ Chonkie.Pipeline/             # Pipeline infrastructure
â”‚   â”œâ”€â”€ Chonkie.Handshakes/           # Base handshake
â”‚   â”‚   â”œâ”€â”€ Chonkie.Handshakes.Chroma/
â”‚   â”‚   â”œâ”€â”€ Chonkie.Handshakes.Qdrant/
â”‚   â”‚   â”œâ”€â”€ Chonkie.Handshakes.Pinecone/
â”‚   â”‚   â”œâ”€â”€ Chonkie.Handshakes.Weaviate/
â”‚   â”‚   â”œâ”€â”€ Chonkie.Handshakes.Pgvector/
â”‚   â”‚   â”œâ”€â”€ Chonkie.Handshakes.MongoDB/
â”‚   â”‚   â”œâ”€â”€ Chonkie.Handshakes.Elastic/
â”‚   â”‚   â””â”€â”€ Chonkie.Handshakes.Turbopuffer/
â”‚   â”œâ”€â”€ Chonkie.Porters/              # Export functionality
â”‚   â””â”€â”€ Chonkie.Genies/               # LLM integrations
â”‚       â”œâ”€â”€ Chonkie.Genies.OpenAI/
â”‚       â”œâ”€â”€ Chonkie.Genies.Azure/
â”‚       â””â”€â”€ Chonkie.Genies.Gemini/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ Chonkie.Core.Tests/
â”‚   â”œâ”€â”€ Chonkie.Tokenizers.Tests/
â”‚   â”œâ”€â”€ Chonkie.Chunkers.Tests/
â”‚   â”œâ”€â”€ Chonkie.Embeddings.Tests/
â”‚   â”œâ”€â”€ Chonkie.Integration.Tests/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ benchmarks/
â”‚   â””â”€â”€ Chonkie.Benchmarks/           # BenchmarkDotNet performance tests
â”œâ”€â”€ samples/
â”‚   â”œâ”€â”€ Chonkie.Samples.BasicUsage/
â”‚   â”œâ”€â”€ Chonkie.Samples.Pipeline/
â”‚   â”œâ”€â”€ Chonkie.Samples.VectorDB/
â”‚   â””â”€â”€ Chonkie.Samples.Advanced/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ guides/
â”‚   â””â”€â”€ migration/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/                    # CI/CD pipelines
â”œâ”€â”€ PORT_PLAN.md                      # This file
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â””â”€â”€ Chonkie.Net.sln
```

### 2.3 Key NuGet Dependencies

#### **Core Dependencies (Minimal)**
```xml
<!-- Logging abstraction -->
<PackageReference Include="Microsoft.Extensions.Logging.Abstractions" Version="8.0.0" />

<!-- Dependency injection -->
<PackageReference Include="Microsoft.Extensions.DependencyInjection.Abstractions" Version="8.0.0" />

<!-- Configuration -->
<PackageReference Include="Microsoft.Extensions.Options" Version="8.0.0" />
```

#### **Optional Dependencies (Feature Packages)**

**Tokenizers:**
```xml
<PackageReference Include="Microsoft.ML.Tokenizers" Version="0.21.0" />
<PackageReference Include="SharpToken" Version="2.0.0" />
```

**Markdown Processing:**
```xml
<PackageReference Include="Markdig" Version="0.37.0" />
```

**ML/ONNX:**
```xml
<PackageReference Include="Microsoft.ML.OnnxRuntime" Version="1.17.0" />
<PackageReference Include="Microsoft.ML.OnnxRuntime.Gpu" Version="1.17.0" />
```

**Embeddings:**
```xml
<PackageReference Include="Azure.AI.OpenAI" Version="1.0.0" />
<PackageReference Include="OpenAI" Version="2.0.0" />
<!-- Provider-specific SDKs as needed -->
```

**Vector Databases:**
```xml
<PackageReference Include="ChromaDB.Client" Version="1.0.0" />
<PackageReference Include="Qdrant.Client" Version="1.9.0" />
<PackageReference Include="Npgsql" Version="8.0.0" />
<!-- Database-specific SDKs -->
```

---

## 3. Core Implementation Details

### 3.1 Type System

#### **Chunk (Primary Type)**
```csharp
namespace Chonkie.Core.Types;

/// <summary>
/// Represents a chunk of text with metadata and optional embeddings.
/// </summary>
public record Chunk
{
    /// <summary>
    /// Unique identifier for the chunk.
    /// </summary>
    public string Id { get; init; } = $"chnk_{Guid.NewGuid():N}";

    /// <summary>
    /// The text content of the chunk.
    /// </summary>
    public required string Text { get; init; }

    /// <summary>
    /// Starting character index in the original text.
    /// </summary>
    public int StartIndex { get; init; }

    /// <summary>
    /// Ending character index in the original text.
    /// </summary>
    public int EndIndex { get; init; }

    /// <summary>
    /// Number of tokens in this chunk.
    /// </summary>
    public int TokenCount { get; init; }

    /// <summary>
    /// Optional context metadata for the chunk.
    /// </summary>
    public string? Context { get; init; }

    /// <summary>
    /// Optional embedding vector for the chunk.
    /// </summary>
    public float[]? Embedding { get; init; }

    /// <summary>
    /// Character length of the chunk text.
    /// </summary>
    public int Length => Text.Length;

    /// <summary>
    /// Converts the chunk to a dictionary representation.
    /// </summary>
    public Dictionary<string, object?> ToDictionary()
    {
        return new Dictionary<string, object?>
        {
            [nameof(Id)] = Id,
            [nameof(Text)] = Text,
            [nameof(StartIndex)] = StartIndex,
            [nameof(EndIndex)] = EndIndex,
            [nameof(TokenCount)] = TokenCount,
            [nameof(Context)] = Context,
            [nameof(Embedding)] = Embedding
        };
    }

    /// <summary>
    /// Creates a shallow copy of the chunk.
    /// </summary>
    public Chunk Copy() => this with { };
}
```

#### **Document**
```csharp
namespace Chonkie.Core.Types;

/// <summary>
/// Represents a document that can be chunked and processed.
/// </summary>
public class Document
{
    /// <summary>
    /// Unique identifier for the document.
    /// </summary>
    public string Id { get; set; } = $"doc_{Guid.NewGuid():N}";

    /// <summary>
    /// The text content of the document.
    /// </summary>
    public required string Content { get; set; }

    /// <summary>
    /// List of chunks extracted from the document.
    /// </summary>
    public List<Chunk> Chunks { get; set; } = new();

    /// <summary>
    /// Metadata associated with the document.
    /// </summary>
    public Dictionary<string, object> Metadata { get; set; } = new();

    /// <summary>
    /// Optional source path or identifier.
    /// </summary>
    public string? Source { get; set; }
}
```

#### **Sentence**
```csharp
namespace Chonkie.Core.Types;

/// <summary>
/// Represents a sentence with position and token information.
/// </summary>
public record Sentence
{
    public required string Text { get; init; }
    public int StartIndex { get; init; }
    public int EndIndex { get; init; }
    public int TokenCount { get; init; }
}
```

### 3.2 Tokenizer Interface

```csharp
namespace Chonkie.Core.Interfaces;

/// <summary>
/// Defines the contract for text tokenization.
/// </summary>
public interface ITokenizer
{
    /// <summary>
    /// Encodes text into a sequence of token IDs.
    /// </summary>
    IReadOnlyList<int> Encode(string text);

    /// <summary>
    /// Decodes a sequence of token IDs back into text.
    /// </summary>
    string Decode(IReadOnlyList<int> tokens);

    /// <summary>
    /// Counts the number of tokens in the given text.
    /// </summary>
    int CountTokens(string text);

    /// <summary>
    /// Encodes multiple texts in a batch for efficiency.
    /// </summary>
    IReadOnlyList<IReadOnlyList<int>> EncodeBatch(IEnumerable<string> texts);

    /// <summary>
    /// Decodes multiple token sequences in a batch.
    /// </summary>
    IReadOnlyList<string> DecodeBatch(IEnumerable<IReadOnlyList<int>> tokenSequences);

    /// <summary>
    /// Counts tokens for multiple texts in a batch.
    /// </summary>
    IReadOnlyList<int> CountTokensBatch(IEnumerable<string> texts);
}
```

### 3.3 Base Chunker

```csharp
namespace Chonkie.Chunkers.Base;

/// <summary>
/// Abstract base class for all chunker implementations.
/// </summary>
public abstract class BaseChunker
{
    protected ITokenizer Tokenizer { get; }
    protected ILogger Logger { get; }
    protected bool UseParallelProcessing { get; set; } = true;

    protected BaseChunker(ITokenizer tokenizer, ILogger logger)
    {
        Tokenizer = tokenizer ?? throw new ArgumentNullException(nameof(tokenizer));
        Logger = logger ?? throw new ArgumentNullException(nameof(logger));
    }

    /// <summary>
    /// Chunks a single text into a list of chunks.
    /// </summary>
    public abstract IReadOnlyList<Chunk> Chunk(string text);

    /// <summary>
    /// Chunks multiple texts in batch, optionally showing progress.
    /// </summary>
    public virtual IReadOnlyList<IReadOnlyList<Chunk>> ChunkBatch(
        IEnumerable<string> texts,
        IProgress<double>? progress = null,
        CancellationToken cancellationToken = default)
    {
        var textList = texts.ToList();
        if (textList.Count == 0)
            return Array.Empty<IReadOnlyList<Chunk>>();

        if (textList.Count == 1)
            return new[] { Chunk(textList[0]) };

        if (UseParallelProcessing)
            return ParallelBatchProcessing(textList, progress, cancellationToken);
        else
            return SequentialBatchProcessing(textList, progress, cancellationToken);
    }

    /// <summary>
    /// Chunks a document and populates its Chunks collection.
    /// </summary>
    public virtual Document ChunkDocument(Document document)
    {
        if (document.Chunks.Any())
        {
            // Re-chunk existing chunks
            var newChunks = new List<Chunk>();
            foreach (var oldChunk in document.Chunks)
            {
                var subChunks = Chunk(oldChunk.Text);
                foreach (var subChunk in subChunks)
                {
                    newChunks.Add(subChunk with
                    {
                        StartIndex = subChunk.StartIndex + oldChunk.StartIndex,
                        EndIndex = subChunk.EndIndex + oldChunk.StartIndex
                    });
                }
            }
            document.Chunks = newChunks;
        }
        else
        {
            document.Chunks = Chunk(document.Content).ToList();
        }

        return document;
    }

    protected virtual IReadOnlyList<IReadOnlyList<Chunk>> SequentialBatchProcessing(
        IReadOnlyList<string> texts,
        IProgress<double>? progress,
        CancellationToken cancellationToken)
    {
        var results = new List<IReadOnlyList<Chunk>>(texts.Count);
        for (int i = 0; i < texts.Count; i++)
        {
            cancellationToken.ThrowIfCancellationRequested();
            results.Add(Chunk(texts[i]));
            progress?.Report((i + 1) / (double)texts.Count);
        }
        return results;
    }

    protected virtual IReadOnlyList<IReadOnlyList<Chunk>> ParallelBatchProcessing(
        IReadOnlyList<string> texts,
        IProgress<double>? progress,
        CancellationToken cancellationToken)
    {
        var results = new ConcurrentBag<(int Index, IReadOnlyList<Chunk> Chunks)>();
        var completed = 0;

        Parallel.ForEach(
            texts.Select((text, index) => (text, index)),
            new ParallelOptions
            {
                CancellationToken = cancellationToken,
                MaxDegreeOfParallelism = Math.Max(1, Environment.ProcessorCount * 3 / 4)
            },
            item =>
            {
                var chunks = Chunk(item.text);
                results.Add((item.index, chunks));
                
                var current = Interlocked.Increment(ref completed);
                progress?.Report(current / (double)texts.Count);
            });

        return results
            .OrderBy(r => r.Index)
            .Select(r => r.Chunks)
            .ToList();
    }
}
```

### 3.4 Pipeline Architecture

```csharp
namespace Chonkie.Pipeline;

/// <summary>
/// Fluent pipeline for composing chunking operations.
/// </summary>
public class Pipeline
{
    private readonly List<IPipelineComponent> _components = new();
    private readonly IServiceProvider _serviceProvider;

    public Pipeline(IServiceProvider? serviceProvider = null)
    {
        _serviceProvider = serviceProvider ?? BuildDefaultServiceProvider();
    }

    public Pipeline ChunkWith(string chunkerType, Action<ChunkerOptions>? configure = null)
    {
        var component = ComponentRegistry.CreateChunker(chunkerType, configure);
        _components.Add(component);
        return this;
    }

    public Pipeline RefineWith(string refineryType, Action<RefineryOptions>? configure = null)
    {
        var component = ComponentRegistry.CreateRefinery(refineryType, configure);
        _components.Add(component);
        return this;
    }

    public Pipeline FetchWith(string fetcherType, Action<FetcherOptions>? configure = null)
    {
        var component = ComponentRegistry.CreateFetcher(fetcherType, configure);
        _components.Add(component);
        return this;
    }

    public async Task<Document> RunAsync(
        string text,
        CancellationToken cancellationToken = default)
    {
        var document = new Document { Content = text };
        
        foreach (var component in _components)
        {
            cancellationToken.ThrowIfCancellationRequested();
            document = await component.ProcessAsync(document, cancellationToken);
        }

        return document;
    }

    private static IServiceProvider BuildDefaultServiceProvider()
    {
        var services = new ServiceCollection();
        // Register default services
        return services.BuildServiceProvider();
    }
}
```

---

## 4. Performance Optimizations

### 4.1 C#-Specific Optimizations

#### **Memory Efficiency**
- Use `Span<T>` and `ReadOnlySpan<T>` for string slicing without allocations
- `Memory<T>` for async scenarios
- `ArrayPool<T>.Shared` for temporary buffers
- `stackalloc` for small, short-lived allocations

#### **String Operations**
```csharp
// Instead of substring allocations
string text = "example";
ReadOnlySpan<char> span = text.AsSpan(0, 3);

// String building
var builder = new StringBuilder(capacity);

// Or for small strings
var handler = new DefaultInterpolatedStringHandler();
```

#### **Collection Performance**
- Pre-allocate collections when size is known: `new List<T>(capacity)`
- Use `CollectionsMarshal.AsSpan()` for list manipulation
- Prefer `IReadOnlyList<T>` over `IEnumerable<T>` when possible

### 4.2 Parallel Processing

#### **Task Parallel Library (TPL)**
```csharp
// Parallel.ForEach with optimal degree of parallelism
var options = new ParallelOptions
{
    MaxDegreeOfParallelism = Environment.ProcessorCount * 3 / 4,
    CancellationToken = cancellationToken
};

Parallel.ForEach(items, options, item => ProcessItem(item));
```

#### **PLINQ**
```csharp
// For data-parallel operations
var results = texts
    .AsParallel()
    .WithDegreeOfParallelism(Environment.ProcessorCount * 3 / 4)
    .WithCancellation(cancellationToken)
    .Select(text => Chunk(text))
    .ToList();
```

### 4.3 Async/Await Best Practices

```csharp
// ValueTask for hot paths that may complete synchronously
public ValueTask<Chunk> ChunkAsync(string text, CancellationToken ct = default)
{
    if (string.IsNullOrEmpty(text))
        return ValueTask.FromResult(Chunk.Empty);
    
    return ChunkAsyncCore(text, ct);
}

// ConfigureAwait(false) in library code
await operation.ConfigureAwait(false);
```

### 4.4 JIT Optimization Hints

```csharp
// Aggressive inlining for hot paths
[MethodImpl(MethodImplOptions.AggressiveInlining)]
private int CountTokensFast(ReadOnlySpan<char> text) { ... }

// Aggressive optimization for entire class
[MethodImpl(MethodImplOptions.AggressiveOptimization)]
public IReadOnlyList<Chunk> Chunk(string text) { ... }
```

### 4.5 SIMD Operations

```csharp
using System.Numerics;
using System.Runtime.Intrinsics;

// For numerical operations on embeddings
public static float CosineSimilarity(ReadOnlySpan<float> a, ReadOnlySpan<float> b)
{
    if (Vector.IsHardwareAccelerated && a.Length >= Vector<float>.Count)
    {
        // Use SIMD operations
        return CosineSimilaritySIMD(a, b);
    }
    return CosineSimilarityScalar(a, b);
}
```

---

## 5. Integration Strategy

### 5.1 Tokenizers

| Python Library | .NET Alternative | Package | Notes |
|----------------|------------------|---------|-------|
| `tokenizers` (HuggingFace) | `Microsoft.ML.Tokenizers` | `Microsoft.ML.Tokenizers` | Official Microsoft library |
| `tiktoken` | `SharpToken` or `TiktokenSharp` | `SharpToken` | OpenAI tokenization |
| `transformers` | `Microsoft.ML.Tokenizers` + ONNX | `Microsoft.ML.OnnxRuntime` | For model-based tokenization |
| Custom functions | Lambda/Func | Built-in | Support custom token counters |

### 5.2 Embeddings Providers

| Provider | .NET SDK | Package | Authentication |
|----------|----------|---------|----------------|
| OpenAI | Community SDK | `OpenAI` or `Betalgo.OpenAI` | API Key |
| Azure OpenAI | Official SDK | `Azure.AI.OpenAI` | API Key or Azure Identity |
| Cohere | Official SDK | `Cohere` | API Key |
| Gemini | Official SDK | `Google.AI.GenerativeAI` | API Key |
| Jina AI | HTTP Client | Built-in `HttpClient` | API Key |
| Voyage AI | HTTP Client | Built-in `HttpClient` | API Key |
| Sentence Transformers | ONNX Runtime | `Microsoft.ML.OnnxRuntime` | Local models |
| Model2Vec | Custom Implementation | TBD | Local models |
| LiteLLM | Community SDK | `LiteLLM` (if available) | Unified API wrapper |
| Catsu | Community SDK | `Catsu` (if available) | Unified client wrapper |

### 5.3 Vector Databases

| Database | .NET SDK | Package | Status |
|----------|----------|---------|--------|
| ChromaDB | Community | `ChromaDB.Client` | Available |
| Qdrant | Official | `Qdrant.Client` | Official support |
| Pinecone | Official | `Pinecone` | Official support |
| Weaviate | Official | `Weaviate.Client` | Official support |
| PostgreSQL (pgvector) | Npgsql + Extension | `Npgsql` + `Pgvector` | Well supported |
| MongoDB | Official | `MongoDB.Driver` | Official support |
| Elasticsearch | Official | `Elastic.Clients.Elasticsearch` | Official support |
| Milvus | Official | `Milvus.Client` | Official support |
| Turbopuffer | HTTP Client | Built-in `HttpClient` | API-based |

### 5.4 LLM Providers

| Provider | .NET SDK | Package | OpenAI Compatible |
|----------|----------|---------|-------------------|
| OpenAI | Multiple options | `Azure.AI.OpenAI` or `OpenAI` | Yes |
| Azure OpenAI | Official | `Azure.AI.OpenAI` | Yes |
| Gemini | Official | `Google.AI.GenerativeAI` | No |
| Anthropic | Community | `Anthropic.SDK` | No (but similar) |
| OpenRouter | OpenAI SDK | `Azure.AI.OpenAI` | Yes |
| Local (Ollama, LM Studio) | OpenAI SDK | `Azure.AI.OpenAI` | Yes |

---

## 6. Migration Phases

### Phase 1: Foundation (Weeks 1-2) âœ… COMPLETE
**Goal:** Establish project structure and core types

#### Tasks
- [x] Set up solution structure and projects
- [x] Configure build system (MSBuild, props files)
- [x] Implement core types
  - [x] `Chunk` record
  - [x] `Document` class
  - [x] `Sentence` record
  - [x] Supporting types (RecursiveLevel, etc.)
- [x] Create tokenizer infrastructure
  - [x] `ITokenizer` interface
  - [x] `CharacterTokenizer` implementation
  - [x] `WordTokenizer` implementation
  - [x] `AutoTokenizer` factory
- [x] Set up logging infrastructure
  - [x] `ILogger` integration (via Microsoft.Extensions.Logging.Abstractions)
  - [x] Structured logging helpers (ready for use)
- [x] Initial unit tests
  - [x] Types tests (Chunk, Document, Sentence)
  - [x] Tokenizer tests (Character, Word, Auto)
- [x] Set up CI/CD (GitHub Actions)
  - [x] Build pipeline
  - [x] Test pipeline

**Success Criteria:**
- âœ… All core types implemented and tested - **50 tests passing**
- âœ… Basic tokenizers working - **CharacterTokenizer, WordTokenizer, AutoTokenizer**
- âœ… CI/CD pipeline green - **GitHub Actions workflow created**
- âœ… Code coverage >70% - **Comprehensive test suite with edge cases**

---

### Phase 2: Core Chunkers (Weeks 3-4) âœ… COMPLETE

**Status:** Complete  
**Timeline:** Week 3-4  
**Date Completed:** October 21, 2025

#### Deliverables âœ…
- [x] `IChunker` interface
- [x] `BaseChunker` abstract class with batch processing
- [x] `TokenChunker` implementation
- [x] `SentenceChunker` implementation
- [x] `RecursiveChunker` implementation
- [x] `RecursiveLevel` and `RecursiveRules` types
- [x] Comprehensive test suite (16 tests)
- [x] Chonkie.Chunkers project

#### Implementation Notes
- All chunkers support both sequential and parallel batch processing
- TokenChunker includes overlap support with configurable size
- SentenceChunker handles sentence boundary detection with multiple delimiters
- RecursiveChunker implements 5-level hierarchy (paragraphs â†’ sentences â†’ pauses â†’ words â†’ tokens)
- Comprehensive logging integration via Microsoft.Extensions.Logging
- 66 total tests passing (100%)
**Goal:** Implement fundamental chunking strategies

#### Tasks
- [ ] Implement `BaseChunker`
  - [ ] Abstract base class
  - [ ] Batch processing (sequential & parallel)
  - [ ] Progress reporting
  - [ ] Document chunking
- [ ] Implement `TokenChunker`
  - [ ] Fixed-size chunking
  - [ ] Overlap support
  - [ ] Batch optimization
- [ ] Implement `SentenceChunker`
  - [ ] Sentence boundary detection
  - [ ] Multiple delimiter support
  - [ ] Minimum sentence constraints
  - [ ] Recipe system integration
- [ ] Implement `RecursiveChunker`
  - [ ] Hierarchical splitting
  - [ ] Custom rule support
  - [ ] Recipe system integration
- [ ] Comprehensive tests
  - [ ] Unit tests for each chunker
  - [ ] Edge case coverage
  - [ ] Integration tests
- [ ] Performance benchmarks
  - [ ] BenchmarkDotNet setup
  - [ ] Compare with Python metrics

**Success Criteria:**
- âœ… All core chunkers implemented
- âœ… Test coverage >80%
- âœ… Benchmarks show competitive performance
- âœ… Documentation complete

---

### Phase 3: Advanced Chunkers (Weeks 5-6) âœ… COMPLETE
**Goal:** Implement specialized chunking strategies

**Status:** Complete  
**Timeline:** Week 5-6  
**Date Started:** October 21, 2025  
**Date Completed:** October 21, 2025

#### Deliverables âœ…
- [x] `SemanticChunker` implementation
  - [x] Embedding integration via IEmbeddings
  - [x] Similarity calculation with window-based approach
  - [x] Threshold-based splitting with local minima detection
  - [x] Skip-and-merge for non-consecutive groups
  - [x] Custom delimiters and sentence splitting
  - [x] 7 comprehensive unit tests
- [x] `LateChunker` implementation
  - [x] Extends RecursiveChunker
  - [x] Embed-then-chunk approach
  - [x] Batch embedding support
  - [x] Maintains all RecursiveChunker functionality
  - [x] 6 comprehensive unit tests
- [x] Optional chunkers documented
  - [x] `CodeChunker` - Research completed, documented in ADVANCED_CHUNKERS.md
  - [x] `NeuralChunker` - ONNX integration path documented
  - [x] `SlumberChunker` - LLM integration approach documented
  - [x] Implementation priorities and recommendations provided
- [x] Comprehensive testing
  - [x] 13 new unit tests (7 Semantic + 6 Late)
  - [x] 239 total tests passing (100%)
  - [x] Edge case coverage for empty/whitespace text
  - [x] Parameter validation tests
- [x] Documentation
  - [x] ADVANCED_CHUNKERS.md created
  - [x] Optional chunker requirements documented
  - [x] .NET alternatives research completed

#### Implementation Notes
- SemanticChunker uses a simplified similarity-based approach without Savitzky-Golay filtering (can be added later if needed)
- LateChunker generates chunk-level embeddings (token-level embeddings would require IEmbeddings interface extension)
- Both chunkers integrate seamlessly with existing IEmbeddings infrastructure
- TestEmbeddings helper class created for unit testing without external dependencies
- All tests use xUnit style consistent with existing test suite (no Moq/FluentAssertions)
- 83 total tests passing (50 Phase 1 + 16 Phase 2 + 13 Phase 3 + 186 Embeddings)

**Success Criteria:**
- âœ… Semantic and Late chunkers fully implemented and working
- âœ… Optional chunkers researched and documented as future work
- âœ… Test coverage excellent (13 new tests, 100% passing)
- âœ… Documentation complete with implementation guidance
- âœ… Code quality maintained with proper error handling

---


### Phase 4: Supporting Infrastructure (Weeks 7-8) âœ… COMPLETE
**Goal:** Build ecosystem components

#### Deliverables âœ…
- [x] `IFetcher` interface and `FileFetcher` implementation (directory traversal, file filtering)
- [x] `IChef` interface, `TextChef`, `MarkdownChef` (Markdig), `TableChef` (optional)
- [x] `IRefinery` interface, `OverlapRefinery`, `EmbeddingsRefinery`
- [x] `IPorter` interface, `JsonPorter` implementation
- [x] Unit tests for all components
- [x] Integration sample demonstrating Fetcher â†’ Chef â†’ Refinery â†’ Porter

#### Implementation Notes
- All infrastructure components are implemented and tested in `src/Chonkie.*` and `tests/Chonkie.*.Tests`.
- Unit tests cover normal, edge, and error cases for each component.
- Integration sample in `samples/Chonkie.Infrastructure.Sample` demonstrates end-to-end usage.
- Test run: 284 total, 240 passed, 44 skipped (integration tests requiring API keys), 0 failed.
- Matches Python reference implementation in architecture and coverage.

#### Verification
- All .NET tests pass; infrastructure is robust and matches Python in features and test coverage.
- Integration tests will run with required environment variables for external APIs.
- No issues detected; ready for next phase.

**Success Criteria:**
- âœ… All infrastructure components working
- âœ… Integration tests passing
- âœ… Test coverage >80%
- âœ… Examples for each component

---

### Phase 5: Embeddings (Weeks 9-10) âœ… COMPLETE
**Goal:** Implement embedding provider integrations

**Status:** Complete  
**Timeline:** Week 9-10  
**Date Completed:** November 2025

#### Deliverables âœ…
- [x] Create embeddings infrastructure
  - [x] `IEmbeddings` interface
  - [x] `BaseEmbeddings` abstract class
  - [x] `AutoEmbeddings` factory
- [x] Implement OpenAI embeddings
  - [x] API integration
  - [x] Batch processing
  - [x] Error handling
- [x] Implement Azure OpenAI embeddings
  - [x] Azure SDK integration
  - [x] Identity-based auth
- [x] Implement Sentence Transformers (ONNX)
  - [x] ONNX Runtime integration
  - [x] Model loading and validation
  - [x] Local inference with pooling strategies
  - [x] Microsoft.ML.Tokenizers integration
- [x] Implement additional providers
  - [x] Cohere
  - [x] Gemini
  - [x] Jina AI
  - [x] Voyage AI
- [x] Tests and documentation
  - [x] 186 comprehensive unit tests
  - [x] Integration tests for all providers
  - [x] Complete API documentation

#### Implementation Notes
- All major embedding providers implemented with full feature parity
- ONNX Sentence Transformers includes advanced features: tokenization, pooling, L2 normalization
- Model2Vec not planned (no .NET library available)
- LiteLLM and Catsu are optional convenience wrappers (Python v1.5.0 additions)
- Test coverage excellent: 186 tests passing

**Success Criteria:**
- âœ… Core embedding providers working
- âœ… AutoEmbeddings factory functional
- âœ… Test coverage >80% (achieved)
- âœ… Complete documentation
- âœ… Usage examples provided

---

### Phase 6: Pipeline (Week 11) âœ… COMPLETE
**Goal:** Implement fluent pipeline API

**Status:** Complete  
**Timeline:** Week 11  
**Date Completed:** December 2025

#### Deliverables âœ…
- [x] Design pipeline architecture
  - [x] `Component` base class
  - [x] `ComponentType` enumeration
  - [x] `ComponentRegistry` for discovery
  - [x] `PipelineComponentAttribute` for metadata
- [x] Implement `ComponentRegistry`
  - [x] Component registration system
  - [x] Factory methods
  - [x] Type-based discovery
- [x] Implement `Pipeline` class
  - [x] Fluent API methods
  - [x] Component chaining
  - [x] Execution engine with async support
  - [x] CHOMP architecture support (Fetcherâ†’Chefâ†’Chunkerâ†’Refineryâ†’Porterâ†’Handshake)
- [x] DI integration
  - [x] Service provider support
  - [x] IOptions pattern for configuration
- [x] Comprehensive tests
  - [x] Unit tests for all components
  - [x] Integration tests
  - [x] Complex pipeline scenarios
- [x] Documentation and samples
  - [x] API documentation
  - [x] Sample project (Chonkie.Pipeline.Sample)

#### Implementation Notes
- Complete CHOMP architecture implementation
- Full .NET DI container integration
- Fluent API for intuitive pipeline composition
- Async/await support throughout
- Comprehensive test coverage

**Success Criteria:**
- âœ… Pipeline system fully functional
- âœ… Fluent API intuitive
- âœ… DI integration seamless
- âœ… Test coverage >80%
- âœ… Complete documentation
- âœ… Working samples

---

### Phase 7: Vector DB Integrations (Weeks 12-14) â¬œ NOT STARTED
**Goal:** Implement vector database handshakes

**Note:** Python v1.5.0 now includes MilvusHandshake (total: 9 handshakes)

#### Tasks
- [ ] Create handshake infrastructure
  - [ ] `IHandshake` interface
  - [ ] Base handshake class
  - [ ] Common utilities
- [ ] Priority integrations (Week 12)
  - [ ] ChromaDB handshake
  - [ ] Qdrant handshake
  - [ ] Pinecone handshake
- [ ] Additional integrations (Week 13)
  - [ ] Weaviate handshake
  - [ ] PostgreSQL (pgvector) handshake
  - [ ] MongoDB handshake
  - [ ] Milvus handshake
- [ ] Final integrations (Week 14)
  - [ ] Elasticsearch handshake
  - [ ] Turbopuffer handshake
- [ ] Tests (per integration)
  - [ ] Unit tests with mocks
  - [ ] Integration tests (Docker)
  - [ ] Connection handling
  - [ ] Error scenarios
- [ ] Documentation
  - [ ] Setup guides per database
  - [ ] Code examples
  - [ ] Best practices

**Success Criteria:**
- âœ… All planned handshakes implemented
- âœ… Integration tests passing (with Docker)
- âœ… Test coverage >70%
- âœ… Complete setup documentation

---

### Phase 8: LLM Genies (Week 15) â¬œ NOT STARTED
**Goal:** Implement LLM provider integrations

#### Tasks
- [ ] Create genie infrastructure
  - [ ] `IGenie` interface
  - [ ] Base genie class
  - [ ] Prompt utilities
- [ ] Implement OpenAI genie
  - [ ] API integration
  - [ ] Streaming support
  - [ ] Error handling
- [ ] Implement Azure OpenAI genie
  - [ ] Azure SDK integration
  - [ ] Identity auth
- [ ] Implement Gemini genie
  - [ ] Google SDK integration
  - [ ] API integration
- [ ] Tests and examples
  - [ ] Unit tests
  - [ ] Integration tests
  - [ ] Usage examples
- [ ] Documentation

**Success Criteria:**
- âœ… All genies implemented
- âœ… OpenRouter compatibility verified
- âœ… Test coverage >75%
- âœ… Examples for each provider

---

### Phase 9: Polish & Documentation (Weeks 16-17) â¬œ NOT STARTED
**Goal:** Finalize library for release

#### Tasks
- [ ] Complete documentation (Week 16)
  - [ ] XML documentation for all public APIs
  - [ ] README.md with examples
  - [ ] Conceptual documentation
  - [ ] Migration guide from Python
  - [ ] API reference (DocFX)
  - [ ] Tutorials and guides
- [ ] Sample projects (Week 16)
  - [ ] Basic usage sample
  - [ ] Pipeline sample
  - [ ] Vector DB integration sample
  - [ ] Advanced scenarios sample
  - [ ] ASP.NET Core integration sample
- [ ] Performance optimization (Week 17)
  - [ ] Profiling (dotMemory, dotTrace)
  - [ ] Identify bottlenecks
  - [ ] Optimize hot paths
  - [ ] Memory allocation reduction
  - [ ] Benchmark comparison with Python
- [ ] Complete test coverage (Week 17)
  - [ ] Aim for >85% overall coverage
  - [ ] Edge cases
  - [ ] Error scenarios
  - [ ] Integration tests
- [ ] NuGet package preparation
  - [ ] Package metadata
  - [ ] Icons and branding
  - [ ] README for NuGet
  - [ ] License information
  - [ ] Version strategy
- [ ] CI/CD enhancements
  - [ ] Release automation
  - [ ] Package publishing pipeline
  - [ ] Documentation deployment

**Success Criteria:**
- âœ… All public APIs documented
- âœ… 5+ sample projects
- âœ… Performance optimized
- âœ… Test coverage >85%
- âœ… NuGet packages ready
- âœ… CI/CD complete

---

### Phase 10: Release (Week 18) â¬œ NOT STARTED
**Goal:** Public release and community launch

#### Tasks
- [ ] Beta testing
  - [ ] Internal testing
  - [ ] Community preview
  - [ ] Feedback collection
  - [ ] Bug fixes
- [ ] Final polish
  - [ ] Code review
  - [ ] Documentation review
  - [ ] Performance verification
  - [ ] Security review
- [ ] NuGet package publishing
  - [ ] Publish to NuGet.org
  - [ ] Verify package metadata
  - [ ] Test package installation
- [ ] Documentation website
  - [ ] Deploy documentation
  - [ ] Domain setup
  - [ ] SEO optimization
- [ ] Announcement
  - [ ] Blog post
  - [ ] GitHub release
  - [ ] Social media
  - [ ] Community forums
  - [ ] Hacker News / Reddit
- [ ] Community engagement
  - [ ] Monitor issues
  - [ ] Answer questions
  - [ ] Collect feedback

**Success Criteria:**
- âœ… Beta testing complete with no critical bugs
- âœ… NuGet packages published
- âœ… Documentation website live
- âœ… Public announcement made
- âœ… Initial community feedback positive

---

## 7. Testing Strategy

### 7.1 Unit Tests

**Framework:** xUnit  
**Mocking:** Moq or NSubstitute  
**Assertions:** FluentAssertions

**Coverage Requirements:**
- Core types: >90%
- Tokenizers: >85%
- Chunkers: >85%
- Embeddings: >80%
- Overall: >85%

**Test Structure:**
```csharp
public class TokenChunkerTests
{
    [Fact]
    public void Chunk_EmptyText_ReturnsEmptyList()
    {
        // Arrange
        var tokenizer = new CharacterTokenizer();
        var chunker = new TokenChunker(tokenizer, chunkSize: 100);
        
        // Act
        var result = chunker.Chunk("");
        
        // Assert
        result.Should().BeEmpty();
    }

    [Theory]
    [InlineData(100, 0)]
    [InlineData(512, 64)]
    [InlineData(2048, 256)]
    public void Chunk_VariousSizes_ProducesCorrectChunks(int chunkSize, int overlap)
    {
        // Test implementation
    }
}
```

### 7.2 Integration Tests

**Scope:**
- Database integrations (with Docker containers)
- External API integrations (with mock servers or test accounts)
- End-to-end pipeline scenarios

**Infrastructure:**
- Testcontainers for .NET (Docker-based testing)
- WireMock.Net for HTTP mocking

### 7.3 Benchmarks

**Framework:** BenchmarkDotNet

**Benchmark Categories:**
- Tokenization performance
- Chunking performance (per chunker type)
- Batch processing performance
- Embedding performance
- Pipeline performance

**Example Benchmark:**
```csharp
[MemoryDiagnoser]
[SimpleJob(RuntimeMoniker.Net80)]
public class TokenChunkerBenchmarks
{
    private readonly TokenChunker _chunker;
    private readonly string _text;

    [GlobalSetup]
    public void Setup()
    {
        _chunker = new TokenChunker(new CharacterTokenizer(), chunkSize: 512);
        _text = File.ReadAllText("sample.txt");
    }

    [Benchmark]
    public IReadOnlyList<Chunk> Chunk_SingleText()
    {
        return _chunker.Chunk(_text);
    }

    [Benchmark]
    public IReadOnlyList<IReadOnlyList<Chunk>> ChunkBatch_100Texts()
    {
        var texts = Enumerable.Repeat(_text, 100);
        return _chunker.ChunkBatch(texts);
    }
}
```

### 7.4 Test Data

**Samples:**
- Small texts (< 100 chars)
- Medium texts (100-1000 chars)
- Large texts (> 10,000 chars)
- Edge cases (empty, single char, unicode, emoji)
- Real-world documents (markdown, code, tables)

---

## 8. Documentation Requirements

### 8.1 Code Documentation

**XML Documentation Comments:**
- All public types
- All public members
- All parameters with `<param>` tags
- Return values with `<returns>` tags
- Exceptions with `<exception>` tags
- Examples with `<example>` tags for key APIs

**Example:**
```csharp
/// <summary>
/// Splits text into overlapping chunks of specified token size.
/// </summary>
/// <param name="text">Input text to be chunked.</param>
/// <returns>A list of <see cref="Chunk"/> objects containing the chunked text and metadata.</returns>
/// <exception cref="ArgumentNullException">Thrown when <paramref name="text"/> is null.</exception>
/// <example>
/// <code>
/// var chunker = new TokenChunker(tokenizer, chunkSize: 512);
/// var chunks = chunker.Chunk("Your text here...");
/// foreach (var chunk in chunks)
/// {
///     Console.WriteLine(chunk.Text);
/// }
/// </code>
/// </example>
public IReadOnlyList<Chunk> Chunk(string text)
```

### 8.2 README.md

**Sections:**
1. Project overview and badges
2. Features
3. Installation instructions
4. Quick start example
5. Core concepts
6. Available chunkers
7. Integrations
8. Examples
9. Performance benchmarks
10. Contributing guidelines
11. License

### 8.3 API Documentation

**Tool:** DocFX

**Structure:**
- API reference (auto-generated from XML comments)
- Conceptual documentation
- Tutorials
- How-to guides
- Migration guide from Python

### 8.4 Samples

**Required Samples:**
1. **Basic Usage** - Simple chunking example
2. **Pipeline** - Multi-stage processing
3. **Vector DB Integration** - Ingesting into ChromaDB/Qdrant
4. **ASP.NET Core** - Using Chonkie in a web API
5. **Advanced** - Custom components and recipes

---

## 9. NuGet Package Strategy

### 9.1 Package Naming Convention

**Format:** `Chonkie.<Component>`

### 9.2 Core Package

**Package:** `Chonkie`  
**Description:** Core chunking library with minimal dependencies

**Includes:**
- Core types
- Base chunkers (Token, Sentence, Recursive)
- Character and Word tokenizers
- Basic refineries
- File fetcher
- Text chef

**Dependencies:**
- Microsoft.Extensions.Logging.Abstractions
- Microsoft.Extensions.DependencyInjection.Abstractions

### 9.3 Optional Packages

| Package | Description | Key Dependencies |
|---------|-------------|------------------|
| `Chonkie.Tokenizers.ML` | HuggingFace tokenizers | Microsoft.ML.Tokenizers |
| `Chonkie.Tokenizers.SharpToken` | OpenAI tiktoken | SharpToken |
| `Chonkie.Chunkers.Semantic` | Semantic chunking | Chonkie.Embeddings |
| `Chonkie.Chunkers.Code` | Code-aware chunking | Tree-sitter or alternative |
| `Chonkie.Chunkers.Neural` | ML-based chunking | Microsoft.ML.OnnxRuntime |
| `Chonkie.Embeddings.OpenAI` | OpenAI embeddings | Azure.AI.OpenAI |
| `Chonkie.Embeddings.Azure` | Azure OpenAI | Azure.AI.OpenAI |
| `Chonkie.Embeddings.Cohere` | Cohere embeddings | Cohere SDK |
| `Chonkie.Embeddings.Gemini` | Gemini embeddings | Google.AI SDK |
| `Chonkie.Embeddings.SentenceTransformers` | Local embeddings | Microsoft.ML.OnnxRuntime |
| `Chonkie.Handshakes.Chroma` | ChromaDB integration | ChromaDB.Client |
| `Chonkie.Handshakes.Qdrant` | Qdrant integration | Qdrant.Client |
| `Chonkie.Handshakes.Pinecone` | Pinecone integration | Pinecone SDK |
| `Chonkie.Handshakes.Weaviate` | Weaviate integration | Weaviate.Client |
| `Chonkie.Handshakes.Pgvector` | PostgreSQL pgvector | Npgsql |
| `Chonkie.Handshakes.MongoDB` | MongoDB integration | MongoDB.Driver |
| `Chonkie.Handshakes.Elastic` | Elasticsearch | Elastic.Clients.Elasticsearch |
| `Chonkie.Genies.OpenAI` | OpenAI LLM | Azure.AI.OpenAI |
| `Chonkie.Genies.Azure` | Azure OpenAI LLM | Azure.AI.OpenAI |
| `Chonkie.Genies.Gemini` | Gemini LLM | Google.AI SDK |
| `Chonkie.Chefs.Markdown` | Markdown processing | Markdig |

### 9.4 Meta Package

**Package:** `Chonkie.All`  
**Description:** Includes all Chonkie packages for convenience (not recommended for production)

### 9.5 Package Metadata

**Common Properties:**
```xml
<PropertyGroup>
  <Authors>Gianni Rosa Gallina</Authors>
  <Company>Gianni Rosa Gallina</Company>
  <PackageProjectUrl>https://github.com/gianni-rg/Chonkie.Net</PackageProjectUrl>
  <PackageLicenseExpression>Apache-2.0</PackageLicenseExpression>
  <PackageIcon>chonkie-icon.png</PackageIcon>
  <PackageReadmeFile>README.md</PackageReadmeFile>
  <PackageTags>chunking;rag;ai;nlp;text-processing;embeddings;vector-database</PackageTags>
  <RepositoryUrl>https://github.com/gianni-rg/Chonkie.Net</RepositoryUrl>
  <RepositoryType>git</RepositoryType>
</PropertyGroup>
```

### 9.6 Versioning Strategy

**Semantic Versioning:** MAJOR.MINOR.PATCH

- **MAJOR:** Breaking changes
- **MINOR:** New features (backward compatible)
- **PATCH:** Bug fixes

**Initial Release:** 1.0.0  
**Beta/Preview:** 0.9.0-beta.1, 0.9.0-rc.1

---

## 10. Key Differences from Python

### 10.1 Language Features

| Aspect | Python | C#/.NET |
|--------|--------|---------|
| **Typing** | Dynamic + type hints | Static with nullable reference types |
| **Memory** | Garbage collected | Garbage collected + manual optimization (Span<T>) |
| **Async** | async/await | async/await with ValueTask |
| **Parallelism** | multiprocessing | Task Parallel Library (TPL) |
| **Packaging** | pip/poetry | NuGet |
| **DI** | Manual or frameworks | Built-in DI container |

### 10.2 Architectural Differences

**Dependency Injection:**
```csharp
// C# - Native DI support
services.AddSingleton<ITokenizer, CharacterTokenizer>();
services.AddScoped<IChunker, TokenChunker>();

var chunker = serviceProvider.GetRequiredService<IChunker>();
```

**Configuration:**
```csharp
// C# - IOptions pattern
services.Configure<TokenChunkerOptions>(configuration.GetSection("Chunker"));

public class TokenChunker
{
    public TokenChunker(IOptions<TokenChunkerOptions> options) { ... }
}
```

**Logging:**
```csharp
// C# - ILogger integration
public class TokenChunker
{
    private readonly ILogger<TokenChunker> _logger;
    
    public TokenChunker(ILogger<TokenChunker> logger)
    {
        _logger = logger;
    }
    
    public void Chunk(string text)
    {
        _logger.LogDebug("Chunking text of length {Length}", text.Length);
    }
}
```

### 10.3 Performance Optimizations

**Memory Efficiency:**
```csharp
// Python: String slicing creates new strings
text[start:end]

// C#: Span<T> for zero-allocation slicing
ReadOnlySpan<char> span = text.AsSpan(start, length);
```

**Parallel Processing:**
```csharp
// Python: multiprocessing with Pool
with Pool(processes=workers) as pool:
    results = pool.map(self.chunk, texts)

// C#: TPL with optimal configuration
Parallel.ForEach(texts, options, text => Chunk(text));
```

### 10.4 API Design

**Fluent API (C# advantage):**
```csharp
var pipeline = new Pipeline()
    .ChunkWith("recursive", opts => opts.ChunkSize = 2048)
    .ChunkWith("semantic", opts => opts.ChunkSize = 512)
    .RefineWith("overlap", opts => opts.ContextSize = 128)
    .RefineWith("embeddings", opts => 
        opts.Model = "sentence-transformers/all-MiniLM-L6-v2");

var doc = await pipeline.RunAsync(text);
```

---

## 11. Success Criteria

### 11.1 Functional Requirements

- âœ… **Feature Parity:** All chunkers from Python version implemented
- âœ… **Integration Parity:** Support for same vector DBs and embedding providers
- âœ… **API Compatibility:** Similar API surface for easy migration

### 11.2 Performance Requirements

- âœ… **Speed:** Within 10% of Python performance (or better)
- âœ… **Memory:** Comparable or better memory usage
- âœ… **Package Size:** Comparable to Python (core < 1MB)

### 11.3 Quality Requirements

- âœ… **Test Coverage:** >85% overall
- âœ… **Documentation:** All public APIs documented
- âœ… **Examples:** 5+ working samples
- âœ… **CI/CD:** Automated build, test, and release

### 11.4 Community Requirements

- âœ… **NuGet Packages:** Published to NuGet.org
- âœ… **Documentation Site:** Live and accessible
- âœ… **GitHub Stars:** Target 100+ in first 3 months
- âœ… **Community Engagement:** Active issue response

---

## 12. Progress Tracking

### 12.1 Overall Progress

**Completion:** 65% (6.5/10 phases - Phase 6 + C# 14 Enhancements)

```
Phase 1:  âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…  100%
Phase 2:  âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…  100%
Phase 3:  âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…  100%
Phase 4:  âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…  100%
Phase 5:  âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…  100%
Phase 6:  âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…  100%
C# 14:    âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…  100% (Extension members, Span conversions, TensorPrimitives)
Phase 7:  â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ  0%
Phase 8:  â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ  0%
Phase 9:  â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ  0%
Phase 10: â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ  0%
```

### 12.2 Current Status

**Current Phase:** Phase 7 - Vector DB Integrations  
**Recent Completion:** C# 14 & .NET 10 Enhancements (Phases 1-5 complete)  
**Next Milestone:** Phase 7 Complete (Week 14)  
**Blockers:** None  
**Last Updated:** December 16, 2025

### 12.3 Recent Achievements (December 2025)

**C# 14 & .NET 10 Enhancement Implementation:**
- âœ… Phase 1-3: Extension members for 7 core interfaces (578 LOC)
- âœ… Phase 4: TensorPrimitives migration (20-35% performance improvement)
- âœ… Phase 5: Extended extension members (batch operations, semantic search)
- âœ… All 538 tests passing (472 passed, 66 skipped, 0 failed)
- âœ… Hardware-accelerated SIMD operations for embeddings
- âœ… Zero-copy text processing with span conversions

### 12.4 Recent Changes

**2025-12-16:**
- âœ… Completed Phase 4: TensorPrimitives migration
- âœ… Completed Phase 5: Extended extension members
- âœ… Added 11 new tests for TensorPrimitives methods
- âœ… Migrated all embedding operations to SIMD-accelerated implementations
- âœ… Archived intermediate implementation documentation
- âœ… Updated all status tracking documents

**Python Version Tracking:** 1.5.0 (up from 1.4.0)
- New features: ByteTokenizer, LiteLLMEmbeddings, CatsuEmbeddings, MilvusHandshake
- Total: 6 tokenizers, 10 embedding providers, 9 vector DB handshakes

### 12.3 Milestones

| Milestone | Target Date | Status |
|-----------|-------------|--------|
| Phase 1 Complete | Week 2 | âœ… Completed |
| Phase 2 Complete | Week 4 | âœ… Completed |
| Phase 3 Complete | Week 6 | âœ… Completed |
| Phase 4 Complete | Week 8 | âœ… Completed |
| Phase 5 Complete | Week 10 | âœ… Completed |
| Phase 6 Complete | Week 11 | âœ… Completed |
| Phase 7 Complete | Week 14 | â¬œ Not Started |
| Phase 8 Complete | Week 15 | â¬œ Not Started |
| Phase 9 Complete | Week 17 | â¬œ Not Started |
| Phase 10 Complete | Week 18 | â¬œ Not Started |
| **v1.0 Release** | **Week 18** | â¬œ **Not Started** |

### 12.4 Recent Changes

**December 16, 2025:**
- ðŸ”„ **Python v1.5.0 Released** - Tracking new features
  - Added ByteTokenizer (byte-level tokenization)
  - Added LiteLLMEmbeddings (unified API for 100+ providers)
  - Added CatsuEmbeddings (unified client for 11+ providers)
  - Added MilvusHandshake (Milvus vector database)
  - Total: 6 tokenizers, 10 embeddings, 9 handshakes

**November-December 2025:**
- âœ… **Phase 6 COMPLETE** - Pipeline System
  - Full CHOMP architecture (Fetcherâ†’Chefâ†’Chunkerâ†’Refineryâ†’Porterâ†’Handshake)
  - ComponentRegistry and fluent API
  - Complete DI integration
  - Comprehensive tests and samples
- âœ… **Phase 5 COMPLETE** - Embeddings
  - All 8 major providers implemented
  - 186 comprehensive tests passing
  - ONNX Sentence Transformers with advanced features
  - Complete API documentation
- âœ… **Phase 4 COMPLETE** - Supporting Infrastructure
  - Chefs, Fetchers, Refineries, Porters
  - 472 total tests (425+ passing, 47 skipped)
  - Integration samples

**October 2025:**
- âœ… **Phase 3 COMPLETE** - Advanced Chunkers (Semantic, Late)
- âœ… **Phase 2 COMPLETE** - Core Chunkers (Token, Sentence, Recursive)
- âœ… **Phase 1 COMPLETE** - Foundation (Core types, Tokenizers, CI/CD)

---

## 13. References

### 13.1 Python Chonkie Resources

- **GitHub:** https://github.com/chonkie-inc/chonkie
- **Documentation:** https://docs.chonkie.ai
- **PyPI:** https://pypi.org/project/chonkie/
- **Discord:** https://discord.gg/vH3SkRqmUz

### 13.2 .NET Resources

- **.NET Documentation:** https://learn.microsoft.com/dotnet/
- **C# Language Reference:** https://learn.microsoft.com/dotnet/csharp/
- **NuGet:** https://www.nuget.org/
- **BenchmarkDotNet:** https://benchmarkdotnet.org/

### 13.3 Related Technologies

- **Microsoft.ML.Tokenizers:** https://github.com/dotnet/machinelearning
- **ONNX Runtime:** https://onnxruntime.ai/
- **Markdig:** https://github.com/xoofx/markdig

---

## Appendix A: Python to C# Quick Reference

### A.1 Common Patterns

| Python | C# |
|--------|-----|
| `def func(param: str) -> int:` | `public int Func(string param)` |
| `@dataclass` | `public record` or `public class` |
| `List[str]` | `List<string>` or `IReadOnlyList<string>` |
| `Dict[str, int]` | `Dictionary<string, int>` |
| `Optional[str]` | `string?` (nullable) |
| `Union[str, int]` | Not directly supported (use polymorphism) |
| `for item in items:` | `foreach (var item in items)` |
| `[x for x in items if condition]` | `items.Where(x => condition).ToList()` |
| `with Pool() as pool:` | `Parallel.ForEach()` or `await Task.WhenAll()` |

### A.2 Async Patterns

| Python | C# |
|--------|-----|
| `async def func():` | `public async Task FuncAsync()` |
| `await something()` | `await something()` |
| `asyncio.gather(*tasks)` | `await Task.WhenAll(tasks)` |

---

**End of Port Plan Document**

*This document will be updated as the project progresses through each phase.*
